{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Environment  \n",
    "* python >= 3.6\n",
    "* pytorch >= 1.0\n",
    "* pandas\n",
    "* nltk\n",
    "* numpy\n",
    "* sklearn\n",
    "* pickle\n",
    "* tqdm\n",
    "* json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 刪除多於資訊 (Remove redundant information)  \n",
    "我們在資料集中保留了許多額外資訊供大家使用，但是在這次的教學中我們並沒有用到全部資訊，因此先將多餘的部分先抽走。  \n",
    "In dataset, we reserved lots of information. But in this tutorial, we don't need them, so we need to discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('../data/task1_trainset.csv', dtype=str)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料切割  (Partition)\n",
    "在訓練時，我們需要有個方法去檢驗訓練結果的好壞，因此需要將訓練資料切成training/validataion set。   \n",
    "While training, we need some method to exam our model's performance, so we divide our training data into training/validataion set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv('trainset.csv',index=False)\n",
    "validset.to_csv('validset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/task1_public_testset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計單字 (Count words)  \n",
    "在訓練時，不能直接將單字直接餵入model，因為它只看得懂數字，因此我們必須把所有的單字抽取出來，並將它們打上編號，做出一個字典來對它們做轉換。\n",
    "We can't feed \"word\" into model directly, since it can only recognize number. So, we need to know the total number of word, and give every word a unique number.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這裡，我們需要借助`nltk`這個library來幫忙做文字切割。當然，你也可以選擇自己寫規則來切割(通常上不建議搞死自己)。  \n",
    "另外，我們也使用了`multiprocessing`來加速處理。  \n",
    "In here, we split words by using `nltk library`. You can write your own rules and split it by yourself, but you won't want to do that, trust me.  \n",
    "Also, we use `multiprocessing` to accelerate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import word_tokenize\n",
    "def collect_words(data_path, n_workers=4):\n",
    "    df = pd.read_csv(data_path, dtype=str)\n",
    "        \n",
    "    sent_list = []\n",
    "    for i in df.iterrows():\n",
    "        sent_list += i[1]['Abstract'].split('$$$')\n",
    "\n",
    "    chunks = [\n",
    "        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n",
    "        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n",
    "    ]\n",
    "    with Pool(n_workers) as pool:\n",
    "        chunks = pool.map_async(word_tokenize, chunks)\n",
    "        words = set(sum(chunks.get(), []))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "words |= collect_words('trainset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad: for padding  \n",
    "unk: for word that didn't in our dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n",
    "for word in words:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dicitonary.pkl','wb') as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料格式化 (Data formatting)  \n",
    "有了字典後，接下來我們要把資料整理成一筆一筆，把input的句子轉成數字，把答案轉成onehot的形式。  \n",
    "這裡，我們一樣使用`multiprocessing`來加入進行。  \n",
    "After building dictionary, that's mapping our sentences into number array, and convert answers to onehot format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n",
    "    onehot = [0,0,0,0,0,0]\n",
    "    for l in labels.split('/'):\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sentence, word_dict):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n",
    "    \n",
    "def get_dataset(data_path, word_dict, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, word_dict):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], word_dict))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, word_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n",
    "    if 'Task 1' in data:\n",
    "        processed['Label'] = [label_to_onehot(label) for label in data['Task 1'].split(' ')]\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset('trainset.csv', word_dict, n_workers=4)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset('validset.csv', word_dict, n_workers=4)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset('testset.csv', word_dict, n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料封裝 (Data packing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了更方便的進行batch training，我們將會借助[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)。  \n",
    "而要將資料放入dataloader，我們需要繼承[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)，撰寫適合這份dataset的class。  \n",
    "`collate_fn`用於batch data的後處理，在`dataloder`將選出的data放進list後會呼叫collate_fn，而我們會在此把sentence padding到同樣的長度，才能夠放入torch tensor (tensor必須為矩陣)。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily training in batch, we'll use `dataloader`, which is a function built in Pytorch[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)  \n",
    "To use datalaoder, we need to packing our data into class `dataset` [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)  \n",
    "`collate_fn` is used for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "class AbstractDataset(Dataset):\n",
    "    def __init__(self, data, pad_idx, max_len = 500):\n",
    "        self.data = data\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_sent = max([len(data['Abstract']) for data in datas])\n",
    "        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n",
    "        batch_abstract = []\n",
    "        batch_label = []\n",
    "        sent_len = []\n",
    "        for data in datas:\n",
    "            # padding abstract to make them in same length\n",
    "            pad_abstract = []\n",
    "            for sentence in data['Abstract']:\n",
    "                if len(sentence) > max_len:\n",
    "                    pad_abstract.append(sentence[:max_len])\n",
    "                else:\n",
    "                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n",
    "            sent_len.append(len(pad_abstract))\n",
    "            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n",
    "            batch_abstract.append(pad_abstract)\n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                pad_label = data['Label']\n",
    "                pad_label.extend([[0]*6]*(max_sent-len(pad_label)))\n",
    "                \n",
    "                batch_label.append(pad_label)\n",
    "        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = AbstractDataset(train, PAD_TOKEN, max_len = 64)\n",
    "validData = AbstractDataset(valid, PAD_TOKEN, max_len = 64)\n",
    "testData = AbstractDataset(test, PAD_TOKEN, max_len = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料處理完成後，接下來就是最重要的核心部分：`Model`。  \n",
    "此次範例中我們以簡單的一層RNN + 兩層Linear layer作為示範。  \n",
    "而為了解決每次的句子長度不一的問題(`linear layer必須是fixed input size`)，因此我們把所有字的hidden_state做平均，讓這一個vector代表這句話。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we're going to implement a simple model, which contain one RNN layer and two fully connected layers (Linear layer). Of course you can make it \"deep\".  \n",
    "To solve variant sentence length problem (`input size in linear layer must be fixed`), we can average all hidden_states, and become one vector. (Perfect!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.embedding_size = 50\n",
    "        self.hidden_dim = 512\n",
    "        self.embedding = nn.Embedding(vocabulary_size, self.embedding_size)\n",
    "        self.sent_rnn = nn.GRU(self.embedding_size,\n",
    "                                self.hidden_dim,\n",
    "                                bidirectional=True,\n",
    "                                batch_first=True)\n",
    "        self.l1 = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "        self.l2 = nn.Linear(self.hidden_dim, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        b,s,w,e = x.shape\n",
    "        x = x.view(b,s*w,e)\n",
    "        x, __ = self.sent_rnn(x)\n",
    "        x = x.view(b,s,w,-1)\n",
    "        x = torch.max(x,dim=2)[0]\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.sigmoid(self.l2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定使用的運算裝置  \n",
    "Designate running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義一個算分公式, 讓我們在training能快速了解model的效能  \n",
    "Define score function, let us easily observe model performance while training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.5\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts = predicts > self.threshold\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.uint8) * predicts).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20) #prevent divided by zero\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def _run_epoch(epoch, training):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        description = 'Train'\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=4)\n",
    "\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0\n",
    "    f1_score = F1()\n",
    "    for i, (x, y, sent_len) in trange:\n",
    "        o_labels, batch_loss = _run_iter(x,y)\n",
    "        if training:\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        f1_score.update(o_labels.cpu(), y)\n",
    "\n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "    if training:\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "    else:\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "\n",
    "def _run_iter(x,y):\n",
    "    abstract = x.to(device)\n",
    "    labels = y.to(device)\n",
    "    o_labels = model(abstract)\n",
    "    l_loss = criteria(o_labels, labels)\n",
    "    return o_labels, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model.pkl.'+str(epoch))\n",
    "    with open('model/history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import json\n",
    "model = simpleNet(len(word_dict))\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "criteria = torch.nn.BCELoss()\n",
    "model.to(device)\n",
    "max_epoch = 6\n",
    "history = {'train':[],'valid':[]}\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    _run_epoch(epoch, True)\n",
    "    _run_epoch(epoch, False)\n",
    "    save(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=64,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=4)\n",
    "trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "prediction = []\n",
    "for i, (x, y, sent_len) in trange:\n",
    "    o_labels = model(x.to(device))\n",
    "    o_labels = o_labels>0.5\n",
    "    for idx, o_label in enumerate(o_labels):\n",
    "        prediction.append(o_label[:sent_len[idx]].to('cpu'))\n",
    "prediction = torch.cat(prediction).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['BACKGROUND'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['OBJECTIVES'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['METHODS'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['RESULTS'] = list(prediction[:,3]) + [0]*redundant\n",
    "        submit['CONCLUSIONS'] = list(prediction[:,4]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,5]) + [0]*redundant\n",
    "    else:\n",
    "        submit['BACKGROUND'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['OBJECTIVES'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['METHODS'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['RESULTS'] = [0]*redundant + list(prediction[:,3])\n",
    "        submit['CONCLUSIONS'] = [0]*redundant + list(prediction[:,4])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,5])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubmitGenerator(prediction,\n",
    "                '../task1_sample_submission.csv', \n",
    "                True, \n",
    "                '../task1_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with open('model/history.json', 'r') as f:\n",
    "    history = json.loads(f.read())\n",
    "    \n",
    "train_loss = [l['loss'] for l in history['train']]\n",
    "valid_loss = [l['loss'] for l in history['valid']]\n",
    "train_f1 = [l['f1'] for l in history['train']]\n",
    "valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('F1 Score')\n",
    "plt.plot(train_f1, label='train')\n",
    "plt.plot(valid_f1, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
